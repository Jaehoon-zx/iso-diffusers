{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "# import sys\n",
    "# sys.path.append(\"../..\")\n",
    "\n",
    "import accelerate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd.functional as A\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.state import AcceleratorState\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from packaging import version\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers.utils import ContextManagers\n",
    "\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.utils import check_min_version, deprecate, is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "\n",
    "from eval import compute_fid, compute_ppl\n",
    "def deepspeed_zero_init_disabled_context_manager():\n",
    "        \"\"\"\n",
    "        returns either a context list that includes one that will disable zero.Init or an empty context list\n",
    "        \"\"\"\n",
    "        deepspeed_plugin = AcceleratorState().deepspeed_plugin if accelerate.state.is_initialized() else None\n",
    "        if deepspeed_plugin is None:\n",
    "            return []\n",
    "\n",
    "        return [deepspeed_plugin.zero3_init_context_manager(enable=False)]\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.state import AcceleratorState\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=1,\n",
    "        mixed_precision=\"fp16\"\n",
    "    )\n",
    "with ContextManagers(deepspeed_zero_init_disabled_context_manager()):\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-2\", subfolder=\"text_encoder\", revision=None\n",
    "    )\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-2\", subfolder=\"vae\", revision=None\n",
    "    )\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2\", subfolder=\"unet\", revision=None\n",
    ")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "       \"stabilityai/stable-diffusion-2\", subfolder=\"tokenizer\", revision=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weight_dtype=torch.float32\n",
    "text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator.unwrap_model(text_encoder).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0589ad2ca6324924a758421a902a4cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-2\",\n",
    "        vae=vae,\n",
    "        text_encoder=text_encoder,\n",
    "        tokenizer=tokenizer,\n",
    "        unet=unet,\n",
    "        safety_checker=None,\n",
    "        revision=None,\n",
    "        torch_dtype=torch.float32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.post_quant_conv.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "t1=np.asarray(['asdfasdfasdf', 'sdfds sdfsdf ssdfsdfsdf', 'sdgasdga sdfadsgweqtwgdfbvx fdbsdfg', ' sadgxvcxbdfhasdfw dv sa xwe fsda fssaea a'])\n",
    "t2=np.asarray(['wqet', 'qwetdsgc sfas sas dacxva', 'sadgasdgadsf', 'asdfadsfa'])\n",
    "def embed(text, pipeline):\n",
    "    text_input = pipeline.tokenizer(\n",
    "        list(text),\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipeline.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        embed = text_encoder(text_input.input_ids)[0]\n",
    "    return embed\n",
    "e1 = embed(t1,pipeline)\n",
    "e2 = embed(t2,pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dimensions(x, n_additional_dims):\n",
    "    for _ in range(n_additional_dims):\n",
    "        x = x.unsqueeze(-1)\n",
    "    return x\n",
    "t = torch.rand(4, dtype=torch.float16)\n",
    "t = add_dimensions(t, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 77, 1024])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 77, 1024])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.lerp(e1.unsqueeze(1), e2.unsqueeze(1), t.float()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "z0 = torch.randn([4, 4, 32, 32])\n",
    "z1 = torch.randn([4, 4, 32, 32])\n",
    "et0 = torch.lerp(e1.unsqueeze(1), e2.unsqueeze(1), t.float()).squeeze(1)\n",
    "et1 = torch.lerp(e1.unsqueeze(1), e2.unsqueeze(1), t.float() + 1e-2).squeeze(1)\n",
    "zt0 = torch.lerp(z0, z1, t.float())\n",
    "zt1 = torch.lerp(z0, z1, t.float() + 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 32, 32])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zt0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 77, 1024])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75714/1549683917.py:1: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  pipeline.unet.in_channels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.unet.in_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.unet.sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0087, -0.0706, -0.0516,  ..., -0.0997,  0.0043, -0.1362],\n",
       "          [-0.1111, -0.0520,  0.1678,  ..., -0.1733, -0.1638, -0.1679],\n",
       "          [-0.0506,  0.0042, -0.2724,  ..., -0.0681, -0.2478, -0.0571],\n",
       "          ...,\n",
       "          [ 0.0924,  0.2228,  0.2411,  ...,  0.0505,  0.0598,  0.0155],\n",
       "          [ 0.0598,  0.0916,  0.1169,  ...,  0.0318,  0.0448, -0.0242],\n",
       "          [ 0.1019,  0.1287,  0.0571,  ...,  0.0519,  0.0274, -0.0466]],\n",
       "\n",
       "         [[-0.0251, -0.1223, -0.0995,  ..., -0.1770, -0.0261, -0.2035],\n",
       "          [-0.1482, -0.0652,  0.2063,  ..., -0.2295, -0.2009, -0.2219],\n",
       "          [-0.0697,  0.0109, -0.2599,  ..., -0.0936, -0.2790, -0.0911],\n",
       "          ...,\n",
       "          [ 0.0352,  0.2389,  0.2541,  ..., -0.1172, -0.0864, -0.0976],\n",
       "          [ 0.0092,  0.0635,  0.0900,  ..., -0.1206, -0.1086, -0.1542],\n",
       "          [ 0.0478,  0.0908,  0.0222,  ..., -0.0905, -0.1130, -0.1633]],\n",
       "\n",
       "         [[-0.0855, -0.1824, -0.1764,  ..., -0.2997, -0.2063, -0.3097],\n",
       "          [-0.1874, -0.1301,  0.0714,  ..., -0.2864, -0.2704, -0.3074],\n",
       "          [-0.1189, -0.0456, -0.3128,  ..., -0.1337, -0.3145, -0.1756],\n",
       "          ...,\n",
       "          [-0.0087,  0.1305,  0.1333,  ..., -0.2785, -0.2681, -0.2380],\n",
       "          [-0.0406, -0.0178,  0.0066,  ..., -0.2896, -0.2756, -0.2856],\n",
       "          [-0.0127,  0.0129, -0.0431,  ..., -0.2486, -0.2724, -0.2751]]],\n",
       "\n",
       "\n",
       "        [[[-0.1075, -0.1008, -0.1299,  ..., -0.2538, -0.2423, -0.1588],\n",
       "          [-0.0787, -0.1277, -0.1410,  ..., -0.1938, -0.1530, -0.1403],\n",
       "          [-0.1326, -0.2346, -0.1075,  ..., -0.1001, -0.2122, -0.0509],\n",
       "          ...,\n",
       "          [-0.0602, -0.0608, -0.2363,  ...,  0.0714,  0.0336, -0.0775],\n",
       "          [-0.1856, -0.1117, -0.1070,  ...,  0.0618,  0.0185, -0.0563],\n",
       "          [-0.1114, -0.0834, -0.1110,  ...,  0.0273,  0.0478, -0.0443]],\n",
       "\n",
       "         [[-0.1976, -0.1948, -0.2370,  ..., -0.3428, -0.3025, -0.2169],\n",
       "          [-0.1551, -0.2040, -0.2274,  ..., -0.2424, -0.1939, -0.1768],\n",
       "          [-0.2190, -0.3031, -0.1793,  ..., -0.1135, -0.2484, -0.0913],\n",
       "          ...,\n",
       "          [-0.1399, -0.1146, -0.3236,  ..., -0.0297, -0.0386, -0.1340],\n",
       "          [-0.2535, -0.1537, -0.1923,  ..., -0.0118, -0.0658, -0.1148],\n",
       "          [-0.1828, -0.1332, -0.1940,  ..., -0.0428, -0.0224, -0.1042]],\n",
       "\n",
       "         [[-0.2599, -0.2721, -0.3074,  ..., -0.4247, -0.4136, -0.3073],\n",
       "          [-0.2317, -0.2775, -0.3034,  ..., -0.3349, -0.2821, -0.2719],\n",
       "          [-0.2682, -0.3637, -0.2653,  ..., -0.2088, -0.3124, -0.1689],\n",
       "          ...,\n",
       "          [-0.2416, -0.2331, -0.4207,  ..., -0.1301, -0.1861, -0.2166],\n",
       "          [-0.3623, -0.2982, -0.3009,  ..., -0.1421, -0.1781, -0.1868],\n",
       "          [-0.2876, -0.2584, -0.2915,  ..., -0.1543, -0.1432, -0.1686]]],\n",
       "\n",
       "\n",
       "        [[[-0.1074, -0.1361, -0.0787,  ..., -0.0659, -0.0765, -0.1474],\n",
       "          [-0.0775, -0.1557, -0.0690,  ..., -0.0564, -0.0657, -0.1305],\n",
       "          [-0.1307, -0.1478, -0.0548,  ..., -0.1001, -0.0782, -0.1071],\n",
       "          ...,\n",
       "          [-0.2191, -0.1345, -0.3644,  ...,  0.1528,  0.1112,  0.1628],\n",
       "          [-0.3145, -0.2140,  0.0272,  ...,  0.1734,  0.1495,  0.1423],\n",
       "          [-0.1316, -0.1247, -0.0961,  ...,  0.1905,  0.1662,  0.1288]],\n",
       "\n",
       "         [[-0.2023, -0.2552, -0.2101,  ..., -0.1392, -0.1176, -0.2229],\n",
       "          [-0.1580, -0.2686, -0.2048,  ..., -0.1194, -0.0929, -0.1826],\n",
       "          [-0.2092, -0.2241, -0.1406,  ..., -0.1621, -0.1216, -0.1638],\n",
       "          ...,\n",
       "          [-0.2249, -0.1835, -0.4173,  ...,  0.0892,  0.0319,  0.1090],\n",
       "          [-0.3283, -0.2570, -0.0103,  ...,  0.1162,  0.0822,  0.0918],\n",
       "          [-0.1186, -0.1170, -0.1088,  ...,  0.1292,  0.1104,  0.0807]],\n",
       "\n",
       "         [[-0.2960, -0.3521, -0.3276,  ..., -0.3001, -0.2884, -0.3463],\n",
       "          [-0.2833, -0.3768, -0.3459,  ..., -0.2891, -0.2933, -0.3492],\n",
       "          [-0.2972, -0.3244, -0.2946,  ..., -0.3173, -0.2992, -0.3140],\n",
       "          ...,\n",
       "          [-0.2764, -0.2087, -0.3725,  ..., -0.0294, -0.0723, -0.0158],\n",
       "          [-0.3354, -0.2978, -0.0405,  ..., -0.0213, -0.0383, -0.0305],\n",
       "          [-0.1573, -0.1703, -0.1504,  ...,  0.0094, -0.0239, -0.0190]]],\n",
       "\n",
       "\n",
       "        [[[-0.0233, -0.1113, -0.0794,  ...,  0.0268,  0.1106,  0.0529],\n",
       "          [-0.1193, -0.0719,  0.1270,  ...,  0.0745,  0.1319,  0.2138],\n",
       "          [-0.0496, -0.0473, -0.2011,  ..., -0.0129, -0.0372,  0.0699],\n",
       "          ...,\n",
       "          [-0.0186,  0.0666,  0.0924,  ...,  0.0749,  0.0521,  0.0217],\n",
       "          [-0.0517,  0.0717,  0.1334,  ...,  0.0655, -0.0525,  0.1085],\n",
       "          [ 0.0269,  0.0356,  0.0170,  ...,  0.0020, -0.0236,  0.0251]],\n",
       "\n",
       "         [[-0.0383, -0.1455, -0.1167,  ..., -0.0589,  0.0409, -0.0366],\n",
       "          [-0.1395, -0.0821,  0.1540,  ..., -0.0177,  0.0713,  0.1575],\n",
       "          [-0.0481, -0.0292, -0.1882,  ..., -0.0957, -0.1218, -0.0332],\n",
       "          ...,\n",
       "          [-0.0823,  0.0191,  0.0466,  ..., -0.0331, -0.0559, -0.0639],\n",
       "          [-0.1067,  0.0339,  0.0823,  ..., -0.0263, -0.1778,  0.0162],\n",
       "          [-0.0223, -0.0059, -0.0379,  ..., -0.0983, -0.1373, -0.0626]],\n",
       "\n",
       "         [[-0.0425, -0.1326, -0.1311,  ..., -0.2153, -0.1239, -0.1510],\n",
       "          [-0.1231, -0.0708,  0.0974,  ..., -0.1814, -0.1341, -0.0312],\n",
       "          [-0.0368, -0.0275, -0.1739,  ..., -0.2182, -0.2752, -0.1768],\n",
       "          ...,\n",
       "          [-0.1127, -0.0459, -0.0446,  ..., -0.1878, -0.2201, -0.2112],\n",
       "          [-0.1356, -0.0467, -0.0036,  ..., -0.1956, -0.3216, -0.1455],\n",
       "          [-0.0757, -0.0807, -0.0923,  ..., -0.2404, -0.2948, -0.2039]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.vae.decoder(zt0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [320, 4, 3, 3], expected input[4, 3, 64, 64] to have 4 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipeline\u001b[39m.\u001b[39;49munet(zt0, torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m4\u001b[39;49m]), \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.conda/envs/diffusers/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/diffusers/lib/python3.10/site-packages/diffusers/models/unet_2d_condition.py:900\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    898\u001b[0m     encoder_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_hid_proj(image_embeds)\n\u001b[1;32m    899\u001b[0m \u001b[39m# 2. pre-process\u001b[39;00m\n\u001b[0;32m--> 900\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_in(sample)\n\u001b[1;32m    902\u001b[0m \u001b[39m# 3. down\u001b[39;00m\n\u001b[1;32m    904\u001b[0m is_controlnet \u001b[39m=\u001b[39m mid_block_additional_residual \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m down_block_additional_residuals \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/diffusers/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/diffusers/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.conda/envs/diffusers/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [320, 4, 3, 3], expected input[4, 3, 64, 64] to have 4 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "pipeline.unet(zt0, torch.tensor([1,2,3,4]), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "sampling_shape = [32, 3, 224, 224]\n",
    "z0 = torch.randn(sampling_shape, device=device)\n",
    "z1 = torch.randn(sampling_shape, device=device)\n",
    "t = torch.rand(sampling_shape[0], device=device)\n",
    "t = add_dimensions(t, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slerp(t, z0, z1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"diffusion_pytorch_model.bin\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like output/sd2-ffhq512-3/checkpoint-69000/unet is not the path to a directory containing a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/diffusers/lib/python3.10/site-packages/diffusers/configuration_utils.py:348\u001b[0m, in \u001b[0;36mConfigMixin.load_config\u001b[0;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m     config_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    349\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    350\u001b[0m         filename\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mconfig_name,\n\u001b[1;32m    351\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    352\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    353\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    354\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    355\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    356\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    357\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    358\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    359\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    360\u001b[0m     )\n\u001b[1;32m    361\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/.conda/envs/diffusers/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    112\u001b[0m \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/diffusers/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'output/sd2-ffhq512-3/checkpoint-69000/unet'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moutput/sd2-ffhq512-3\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstabilityai/stable-diffusion-2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m unet \u001b[39m=\u001b[39m UNet2DConditionModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_path \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/checkpoint-69000/unet\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m pipe \u001b[39m=\u001b[39m DiffusionPipeline\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      9\u001b[0m     model_name, \n\u001b[1;32m     10\u001b[0m     unet\u001b[39m=\u001b[39munet, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     custom_pipeline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minterpolate_stable_diffusion\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/diffusers/lib/python3.10/site-packages/diffusers/models/modeling_utils.py:511\u001b[0m, in \u001b[0;36mModelMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m user_agent \u001b[39m=\u001b[39m {\n\u001b[1;32m    505\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdiffusers\u001b[39m\u001b[39m\"\u001b[39m: __version__,\n\u001b[1;32m    506\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfile_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    507\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mframework\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpytorch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    508\u001b[0m }\n\u001b[1;32m    510\u001b[0m \u001b[39m# load config\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m config, unused_kwargs, commit_hash \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload_config(\n\u001b[1;32m    512\u001b[0m     config_path,\n\u001b[1;32m    513\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    514\u001b[0m     return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    515\u001b[0m     return_commit_hash\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    516\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    517\u001b[0m     resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    518\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    519\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    520\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    521\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    522\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    523\u001b[0m     device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m    524\u001b[0m     max_memory\u001b[39m=\u001b[39;49mmax_memory,\n\u001b[1;32m    525\u001b[0m     offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m    526\u001b[0m     offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m    527\u001b[0m     user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    528\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    529\u001b[0m )\n\u001b[1;32m    531\u001b[0m \u001b[39m# load model\u001b[39;00m\n\u001b[1;32m    532\u001b[0m model_file \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/diffusers/lib/python3.10/site-packages/diffusers/configuration_utils.py:384\u001b[0m, in \u001b[0;36mConfigMixin.load_config\u001b[0;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    380\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThere was a specific connection error when trying to load\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00merr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m     )\n\u001b[1;32m    383\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    385\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWe couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt connect to \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to load this model, couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find it\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m in the cached files and it looks like \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m is not the path to a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    387\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m directory containing a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mconfig_name\u001b[39m}\u001b[39;00m\u001b[39m file.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCheckout your internet connection or see how to\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m run the library in offline mode at\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/docs/diffusers/installation#offline-mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    390\u001b[0m     )\n\u001b[1;32m    391\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    393\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load config for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    394\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    395\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mconfig_name\u001b[39m}\u001b[39;00m\u001b[39m file\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like output/sd2-ffhq512-3/checkpoint-69000/unet is not the path to a directory containing a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline, DDPMPipeline, UNet2DConditionModel\n",
    "\n",
    "# Load from my checkpoint/unet\n",
    "model_path = \"output/sd2-ffhq512-3\"\n",
    "model_name = \"stabilityai/stable-diffusion-2\"\n",
    "unet = UNet2DConditionModel.from_pretrained(model_path + \"/checkpoint-69000/unet\")\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    model_name, \n",
    "    unet=unet, \n",
    "    #torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    "    custom_pipeline=\"interpolate_stable_diffusion\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export MODEL_NAME=\"stabilityai/stable-diffusion-2\" #\"CompVis/stable-diffusion-v1-4\"\n",
    "export DATASET_NAME=\"Ryan-sjtu/ffhq512-caption\" #\"facebook/winoground\" #\"lambdalabs/pokemon-blip-captions\"\n",
    "\n",
    "accelerate launch --mixed_precision=\"fp16\"  train_text_to_image.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --dataset_name=$DATASET_NAME \\\n",
    "  --use_ema \\\n",
    "  --resolution=512 --center_crop --random_flip \\\n",
    "  --train_batch_size=4 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --gradient_checkpointing \\\n",
    "  --max_train_steps=70000 \\\n",
    "  --learning_rate=1e-05 \\\n",
    "  --max_grad_norm=1 \\\n",
    "  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n",
    "  --output_dir=\"output/sd2-ffhq512-8\" \\\n",
    "  --validation_epochs=1 \\\n",
    "  --validation_prompts \"a photography of a happy baby\" \"a photography of a woman smiling\" \\\n",
    "  --prompts_reps=8 \\\n",
    "  --image_column=\"image\" \\\n",
    "  --caption_column=\"text\" \\\n",
    "  --split=\"train\" \\\n",
    "  --prediction_type=\"epsilon\" \\\n",
    "  --lambda_pl=1 \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/junho/.cache/huggingface/datasets/Ryan-sjtu___parquet/Ryan-sjtu--ffhq512-caption-168cfb2a7611a8ef/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa24e89155c9464685aff8ddc853f7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name = \"Ryan-sjtu/ffhq512-caption\"\n",
    "dataset = load_dataset(\n",
    "            dataset_name,\n",
    "            None,\n",
    "            cache_dir=None,\n",
    "            use_auth_token=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'text'],\n",
       "    num_rows: 70000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.state import AcceleratorState\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=1,\n",
    "        mixed_precision=\"fp16\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Accelerator' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accelerator\u001b[39m.\u001b[39;49mkeys()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Accelerator' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/junho/projects/junho/iso-diffusers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"examples/text_to_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/junho/.cache/huggingface/datasets/Ryan-sjtu___parquet/Ryan-sjtu--ffhq512-caption-168cfb2a7611a8ef/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5c9bcdc9e44443b0a672038fd03600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from accelerate.state import AcceleratorState\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from dnnlib.util import open_url\n",
    "import torch_utils\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--path', type=str, default=\"Ryan-sjtu/ffhq512-caption\")\n",
    "parser.add_argument('--pretrained_model_name_or_path', type=str, default=\"stabilityai/stable-diffusion-2\")\n",
    "parser.add_argument('--batch_size', type=int, default=128)\n",
    "parser.add_argument('--resolution', type=int, default=256)\n",
    "parser.add_argument('--center_crop', type=bool, default=True)\n",
    "parser.add_argument('--random_flip', type=bool, default=True)\n",
    "parser.add_argument('--fid_dir', type=str, default='./')\n",
    "parser.add_argument('--split', type=str, default='train')\n",
    "parser.add_argument('--file', type=str, default=\"ffhq.npz\")\n",
    "parser.add_argument('--max_samples', type=int, default=20)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "pre_dataset = load_dataset(\n",
    "            args.path,\n",
    "            None,\n",
    "            cache_dir=None,\n",
    "            use_auth_token=True\n",
    "        )\n",
    "column_names = pre_dataset[args.split].column_names # \"train\"\n",
    "\n",
    "DATASET_NAME_MAPPING = {\n",
    "\"lambdalabs/pokemon-blip-captions\": (\"image\", \"text\"),\n",
    "}\n",
    "\n",
    "# 6. Get the column names for input/target.\n",
    "dataset_columns = DATASET_NAME_MAPPING.get(args.path, None)\n",
    "\n",
    "image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
    "\n",
    "caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=None\n",
    ")\n",
    "# Preprocessing the datasets.\n",
    "# We need to tokenize input captions and transform the images.\n",
    "def tokenize_captions(examples, is_train=True):\n",
    "    captions = []\n",
    "    for caption in examples[caption_column]:\n",
    "        if isinstance(caption, str):\n",
    "            captions.append(caption)\n",
    "        elif isinstance(caption, (list, np.ndarray)):\n",
    "            # take a random caption if there are multiple\n",
    "            captions.append(random.choice(caption) if is_train else caption[0])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n",
    "            )\n",
    "    inputs = tokenizer(\n",
    "        captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    return inputs.input_ids\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\n",
    "        transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def preprocess_train(examples):\n",
    "    images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "    examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
    "    examples[\"input_ids\"] = tokenize_captions(examples)\n",
    "    return examples\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# dataset = ImageFolderDataset(args.path)\n",
    "dataset = pre_dataset[args.split].with_transform(preprocess_train)\n",
    "# queue = torch.utils.data.DataLoader(dataset=dataset, batch_size=args.batch_size, pin_memory=True, num_workers=0)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
    "\n",
    "queue  = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=0,\n",
    ")\n",
    "with open_url('https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/inception-2015-12-05.pkl') as f:\n",
    "    model = pickle.load(f).to(device)\n",
    "    model.eval()\n",
    "# accelerator_project_config = ProjectConfiguration(project_dir=\"output/sd2-ffhq512-8\", logging_dir=\"logs\")\n",
    "# accelerator = Accelerator(\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     mixed_precision=\"fp16\",\n",
    "#     project_config=accelerator_project_config,\n",
    "# )\n",
    "# queue = accelerator.prepare(queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(dl, model, batch_size, device, max_samples, include_step=True):\n",
    "    pred_arr = []\n",
    "    total_processed = 0\n",
    "\n",
    "    print('Starting to sample.')\n",
    "    if include_step:\n",
    "        for step, batch in enumerate(dl):\n",
    "            batch = batch[\"pixel_values\"].to(torch.float16)\n",
    "            # ignore labels\n",
    "            if isinstance(batch, list):\n",
    "                batch = batch[0]\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            if batch.shape[1] == 1:  # if image is gray scale\n",
    "                batch = batch.repeat(1, 3, 1, 1)\n",
    "            elif len(batch.shape) == 3:  # if image is gray scale\n",
    "                batch = batch.unsqueeze(1).repeat(1, 3, 1, 1)\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = model(batch, return_features=True).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "            pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
    "            pred_arr.append(pred)\n",
    "            total_processed += pred.shape[0]\n",
    "            if max_samples is not None and total_processed > max_samples:\n",
    "                print('Max of %d samples reached.' % max_samples)\n",
    "                break\n",
    "        pred_arr = np.concatenate(pred_arr, axis=0)\n",
    "        if max_samples is not None:\n",
    "            pred_arr = pred_arr[:max_samples]\n",
    "    else:\n",
    "        for batch in dl:\n",
    "            # ignore labels\n",
    "            if isinstance(batch, list):\n",
    "                batch = batch[0]\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            if batch.shape[1] == 1:  # if image is gray scale\n",
    "                batch = batch.repeat(1, 3, 1, 1)\n",
    "            elif len(batch.shape) == 3:  # if image is gray scale\n",
    "                batch = batch.unsqueeze(1).repeat(1, 3, 1, 1)\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = model(batch, return_features=True).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "            pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
    "            pred_arr.append(pred)\n",
    "            total_processed += pred.shape[0]\n",
    "            if max_samples is not None and total_processed > max_samples:\n",
    "                print('Max of %d samples reached.' % max_samples)\n",
    "                break\n",
    "\n",
    "        pred_arr = np.concatenate(pred_arr, axis=0)\n",
    "        if max_samples is not None:\n",
    "            pred_arr = pred_arr[:max_samples]\n",
    "\n",
    "    return pred_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to sample.\n",
      "Max of 20 samples reached.\n"
     ]
    }
   ],
   "source": [
    "act = get_activations(queue, model, batch_size=args.batch_size, device=device, max_samples=20)\n",
    "mu = np.mean(act, axis=0)\n",
    "sigma = np.cov(act, rowvar=False)\n",
    "file_path = os.path.join(args.fid_dir, args.file)\n",
    "np.savez(file_path, mu=mu, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to sample.\n",
      "Max of 20 samples reached.\n"
     ]
    }
   ],
   "source": [
    "act = get_activations(queue, model, batch_size=args.batch_size, device=device, max_samples=args.max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(queue):\n",
    "    break\n",
    "batch = batch[\"pixel_values\"].to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1., dtype=torch.float16)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "StableDiffusionPipeline.from_pretrained(\n",
    "        args.pretrained_model_name_or_path,\n",
    "        vae=accelerator.unwrap_model(vae),\n",
    "        text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "        tokenizer=tokenizer,\n",
    "        unet=accelerator.unwrap_model(unet),\n",
    "        safety_checker=None,\n",
    "        revision=args.revision,\n",
    "        torch_dtype=weight_dtype,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "diffusers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
